#!/usr/bin/env python3
"""
LLM Integration Test Script

This script tests the OpenAI LLM integration for rule generation.
"""

import asyncio
import json
from app.services.llm_service import LLMService, RuleGenerationRequest as LLMRequest
from app.services.rule_generation_service import rule_generation_service, RuleGenerationRequest
from app.core.llm_rule_context import RuleType


async def test_llm_service():
    """Test the LLM service directly"""
    print("ğŸ§ª Testing LLM Service")
    print("=" * 50)
    
    llm_service = LLMService()
    
    if not llm_service.client:
        print("âŒ LLM service not configured or unavailable")
        return False
    
    print(f"âœ… LLM service initialized with model: {llm_service.config.model}")
    print(f"   API Key configured: {'***' + llm_service.config.api_key[-4:] if llm_service.config.api_key else 'No'}")
    
    # Test completion rule generation
    print("\nğŸ“ Testing Completion Rule Generation")
    
    completion_request = LLMRequest(
        description="æ ¹æ®ä¾›åº”å•†åç§°ä»æ•°æ®åº“æŸ¥è¯¢ç¨å·ï¼Œç”¨äºè¡¥å…¨ç¼ºå¤±çš„ä¾›åº”å•†ç¨å·å­—æ®µ",
        rule_type="completion",
        context=None,
        examples=[]
    )
    
    try:
        result = await llm_service.generate_rule(completion_request)
        
        if result["success"]:
            rule_data = result["data"]
            print(f"âœ… Completion rule generated successfully:")
            print(f"   Rule Name: {rule_data['rule_name']}")
            print(f"   Target Field: {rule_data.get('target_field', 'N/A')}")
            print(f"   Expression: {rule_data['rule_expression']}")
            print(f"   Priority: {rule_data.get('priority', 'N/A')}")
        else:
            print(f"âŒ Completion rule generation failed: {result.get('error')}")
            return False
            
    except Exception as e:
        print(f"âŒ Completion rule generation error: {str(e)}")
        return False
    
    # Test validation rule generation
    print("\nğŸ” Testing Validation Rule Generation") 
    
    validation_request = LLMRequest(
        description="éªŒè¯ä¾›åº”å•†ç¨å·æ ¼å¼æ˜¯å¦æ­£ç¡®ï¼Œç¨å·åº”è¯¥æ˜¯15ä½æ•°å­—åŠ 3ä½å­—æ¯æ•°å­—ç»„åˆ",
        rule_type="validation",
        context=None,
        examples=[]
    )
    
    try:
        result = await llm_service.generate_rule(validation_request)
        
        if result["success"]:
            rule_data = result["data"]
            print(f"âœ… Validation rule generated successfully:")
            print(f"   Rule Name: {rule_data['rule_name']}")
            print(f"   Field Path: {rule_data.get('field_path', 'N/A')}")
            print(f"   Expression: {rule_data['rule_expression']}")
            print(f"   Error Message: {rule_data.get('error_message', 'N/A')}")
        else:
            print(f"âŒ Validation rule generation failed: {result.get('error')}")
            return False
            
    except Exception as e:
        print(f"âŒ Validation rule generation error: {str(e)}")
        return False
    
    return True


async def test_rule_generation_service():
    """Test the integrated rule generation service"""
    print("\n\nğŸš€ Testing Integrated Rule Generation Service")
    print("=" * 50)
    
    # Test completion rule
    print("\nğŸ“ Testing Completion Rule via Service")
    
    completion_request = RuleGenerationRequest(
        rule_type=RuleType.COMPLETION,
        target_field="supplier.tax_no",
        description="å½“ä¾›åº”å•†ç¨å·ä¸ºç©ºæ—¶ï¼Œæ ¹æ®ä¾›åº”å•†åç§°ä»ä¼ä¸šæ•°æ®åº“ä¸­æŸ¥è¯¢å¯¹åº”çš„ç¨å·è¿›è¡Œè¡¥å…¨",
        priority=95
    )
    
    try:
        suggestions = await rule_generation_service.generate_rule_suggestions(completion_request)
        
        if suggestions:
            rule = suggestions[0]
            print(f"âœ… Service completion rule generated:")
            print(f"   ID: {rule.id}")
            print(f"   Name: {rule.rule_name}")
            print(f"   Expression: {rule.rule_expression}")
            print(f"   Confidence: {rule.confidence_score:.1%}")
            print(f"   Explanation: {rule.explanation}")
        else:
            print("âŒ No completion rules generated")
            return False
            
    except Exception as e:
        print(f"âŒ Service completion rule error: {str(e)}")
        return False
    
    # Test validation rule
    print("\nğŸ” Testing Validation Rule via Service")
    
    validation_request = RuleGenerationRequest(
        rule_type=RuleType.VALIDATION,
        field_path="total_amount",
        description="éªŒè¯å‘ç¥¨æ€»é‡‘é¢å¿…é¡»å¤§äº0ï¼Œç¡®ä¿æ‰€æœ‰å‘ç¥¨éƒ½æœ‰æœ‰æ•ˆçš„é‡‘é¢",
        error_message="å‘ç¥¨æ€»é‡‘é¢å¿…é¡»å¤§äº0",
        priority=90
    )
    
    try:
        suggestions = await rule_generation_service.generate_rule_suggestions(validation_request)
        
        if suggestions:
            rule = suggestions[0]
            print(f"âœ… Service validation rule generated:")
            print(f"   ID: {rule.id}")
            print(f"   Name: {rule.rule_name}")
            print(f"   Expression: {rule.rule_expression}")
            print(f"   Error Message: {rule.error_message}")
            print(f"   Confidence: {rule.confidence_score:.1%}")
        else:
            print("âŒ No validation rules generated")
            return False
            
    except Exception as e:
        print(f"âŒ Service validation rule error: {str(e)}")
        return False
    
    return True


async def test_complex_scenarios():
    """Test more complex rule generation scenarios"""
    print("\n\nğŸ¯ Testing Complex Scenarios")
    print("=" * 50)
    
    scenarios = [
        {
            "name": "åŠ¨æ€ç¨ç‡è®¡ç®—",
            "type": RuleType.COMPLETION,
            "description": "æ ¹æ®ä¾›åº”å•†çš„è¡Œä¸šåˆ†ç±»ï¼ˆTECHã€TRAVELã€GENERALç­‰ï¼‰ä»ç¨ç‡è¡¨ä¸­æŸ¥è¯¢å¯¹åº”çš„ç¨ç‡ï¼Œç„¶åè®¡ç®—å‘ç¥¨ç¨é¢ã€‚å¦‚æœæŸ¥è¯¢ä¸åˆ°åˆ™ä½¿ç”¨é»˜è®¤6%ç¨ç‡ã€‚",
            "target_field": "tax_amount"
        },
        {
            "name": "æ—…æ¸¸æœåŠ¡å‘ç¥¨é¡¹ç›®æ ¡éªŒ",
            "type": RuleType.VALIDATION,
            "description": "å½“ä¾›åº”å•†åˆ†ç±»ä¸ºTRAVEL_SERVICEæ—¶ï¼ŒéªŒè¯å‘ç¥¨é¡¹ç›®æè¿°å¿…é¡»åŒ…å«æ ‡å‡†çš„æ—…æ¸¸æœåŠ¡é¡¹ç›®ï¼Œå¦‚ä½å®¿ã€é¤é¥®ã€äº¤é€šè´¹ç­‰ã€‚",
            "field_path": "items",
            "error_message": "æ—…æ¸¸æœåŠ¡å‘ç¥¨é¡¹ç›®æè¿°ä¸è§„èŒƒï¼Œåº”åŒ…å«ä½å®¿ã€é¤é¥®ã€äº¤é€šç­‰æ ‡å‡†æœåŠ¡é¡¹ç›®"
        },
        {
            "name": "å¤§é¢å‘ç¥¨å®¢æˆ·ç¨å·å¿…å¡«",
            "type": RuleType.VALIDATION,
            "description": "å½“å‘ç¥¨æ€»é‡‘é¢è¶…è¿‡5000å…ƒæ—¶ï¼Œå®¢æˆ·ç¨å·å¿…é¡»å¡«å†™ä¸”ä¸èƒ½ä¸ºç©ºï¼Œä»¥æ»¡è¶³ç¨åŠ¡åˆè§„è¦æ±‚ã€‚",
            "field_path": "customer.tax_no",
            "error_message": "é‡‘é¢è¶…è¿‡5000å…ƒçš„å‘ç¥¨ï¼Œå®¢æˆ·ç¨å·å¿…å¡«"
        }
    ]
    
    for i, scenario in enumerate(scenarios, 1):
        print(f"\nğŸ§ª Scenario {i}: {scenario['name']}")
        
        request = RuleGenerationRequest(
            rule_type=scenario["type"],
            target_field=scenario.get("target_field"),
            field_path=scenario.get("field_path"),
            description=scenario["description"],
            error_message=scenario.get("error_message"),
            priority=80
        )
        
        try:
            suggestions = await rule_generation_service.generate_rule_suggestions(request)
            
            if suggestions:
                rule = suggestions[0]
                print(f"   âœ… Generated: {rule.rule_name}")
                print(f"   Expression: {rule.rule_expression[:100]}...")
                if rule.error_message:
                    print(f"   Error: {rule.error_message}")
            else:
                print(f"   âŒ No rules generated for scenario {i}")
                return False
                
        except Exception as e:
            print(f"   âŒ Error in scenario {i}: {str(e)}")
            return False
    
    return True


async def main():
    """Run all tests"""
    print("ğŸš€ Starting LLM Integration Tests")
    print("=" * 80)
    
    results = []
    
    # Test 1: Direct LLM service
    try:
        result1 = await test_llm_service()
        results.append(("LLM Service", result1))
    except Exception as e:
        print(f"âŒ LLM Service test failed: {str(e)}")
        results.append(("LLM Service", False))
    
    # Test 2: Integrated service
    try:
        result2 = await test_rule_generation_service()
        results.append(("Rule Generation Service", result2))
    except Exception as e:
        print(f"âŒ Rule Generation Service test failed: {str(e)}")
        results.append(("Rule Generation Service", False))
    
    # Test 3: Complex scenarios
    try:
        result3 = await test_complex_scenarios()
        results.append(("Complex Scenarios", result3))
    except Exception as e:
        print(f"âŒ Complex scenarios test failed: {str(e)}")
        results.append(("Complex Scenarios", False))
    
    # Summary
    print("\n" + "=" * 80)
    print("ğŸ“Š Test Results Summary")
    print("=" * 80)
    
    passed = sum(1 for _, result in results if result)
    total = len(results)
    
    for test_name, result in results:
        status = "âœ… PASSED" if result else "âŒ FAILED"
        print(f"{test_name:<25} {status}")
    
    print(f"\nOverall: {passed}/{total} tests passed")
    
    if passed == total:
        print("ğŸ‰ All LLM integration tests passed!")
        print("\nğŸ¯ Next Steps:")
        print("1. The LLM integration is working properly")
        print("2. Rules are being generated using GPT-4o-mini")
        print("3. System falls back to templates if LLM fails")
        print("4. Ready for production use!")
    else:
        print("âš ï¸  Some tests failed. Please check the implementation or API configuration.")
    
    return passed == total


if __name__ == "__main__":
    success = asyncio.run(main())
    exit(0 if success else 1)